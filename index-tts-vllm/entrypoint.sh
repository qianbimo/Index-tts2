#!/usr/bin/env bash
set -e

# ====================================================
# IndexTTS-2 vLLM Docker å¯åŠ¨è„šæœ¬ (CUDA 12.4 + uv ä¼˜åŒ–ç‰ˆ)
# ====================================================

# ---------------- å‚æ•°ä¸é»˜è®¤å€¼ ----------------
MODEL_DIR=${MODEL_DIR:-"/app/checkpoints"}
MODEL=${MODEL:-"IndexTeam/IndexTTS-2-vLLM"}
VLLM_USE_MODELSCOPE=${VLLM_USE_MODELSCOPE:-1}
DOWNLOAD_MODEL=${DOWNLOAD_MODEL:-1}
CONVERT_MODEL=${CONVERT_MODEL:-1}
GPU_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION:-0.25}
PORT=${PORT:-7861}
PYPI_MIRROR=${PYPI_MIRROR:-"https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple"}

echo "===================================================="
echo "ğŸš€ å¯åŠ¨ IndexTTS-2 vLLM æœåŠ¡"
echo "ğŸ“ æ¨¡å‹ç›®å½•: $MODEL_DIR"
echo "ğŸ§  æ¨¡å‹åç§°: $MODEL"
echo "ğŸŒ ä½¿ç”¨ ModelScope: $VLLM_USE_MODELSCOPE"
echo "â¬‡ï¸ å…è®¸ä¸‹è½½æ¨¡å‹: $DOWNLOAD_MODEL"
echo "ğŸ”„ è½¬æ¢æ¨¡å‹: $CONVERT_MODEL"
echo "ğŸ§® GPU æ˜¾å­˜å æ¯”: $GPU_MEMORY_UTILIZATION"
echo "ğŸŒ æœåŠ¡ç«¯å£: $PORT"
echo "ğŸ“¦ PyPI é•œåƒæº: $PYPI_MIRROR"
echo "===================================================="

# ---------------- å‡½æ•°å®šä¹‰ ----------------
log() { echo -e "\033[1;32m[INFO]\033[0m $1"; }
warn() { echo -e "\033[1;33m[WARN]\033[0m $1"; }
error() { echo -e "\033[1;31m[ERROR]\033[0m $1"; }

check_model_exists() {
    if [ ! -d "$MODEL_DIR" ]; then
        warn "æ¨¡å‹ç›®å½• $MODEL_DIR ä¸å­˜åœ¨"
        return 1
    fi
    if [ ! -f "$MODEL_DIR/.download_complete" ]; then
        warn "æ¨¡å‹æœªå®Œæˆä¸‹è½½"
        return 1
    fi
    # æ£€æŸ¥åŸºæœ¬æ–‡ä»¶
    if [ ! -f "$MODEL_DIR/tokenizer.json" ] && [ ! -f "$MODEL_DIR/config.json" ]; then
        warn "æ¨¡å‹æ–‡ä»¶ç¼ºå¤± (tokenizer/config)"
        return 1
    fi
    return 0
}

download_from_modelscope() {
    log "ğŸ“¦ ä» ModelScope ä¸‹è½½æ¨¡å‹: $MODEL"
    mkdir -p "$MODEL_DIR"
    modelscope download --model "$MODEL" --local_dir "$MODEL_DIR" || {
        error "ModelScope ä¸‹è½½å¤±è´¥"; exit 1;
    }
    touch "$MODEL_DIR/.download_complete"
}

download_from_huggingface() {
    log "ğŸ“¦ ä» HuggingFace ä¸‹è½½æ¨¡å‹: $MODEL"
    mkdir -p "$MODEL_DIR"
    huggingface-cli download "$MODEL" --local-dir "$MODEL_DIR" --local-dir-use-symlinks False || {
        error "HuggingFace ä¸‹è½½å¤±è´¥"; exit 1;
    }
    touch "$MODEL_DIR/.download_complete"
}

convert_model_format() {
    log "ğŸ”„ æ­£åœ¨è½¬æ¢æ¨¡å‹æ ¼å¼..."
    $PYTHON_CMD /app/convert_hf_format.sh "$MODEL_DIR" || warn "è½¬æ¢è„šæœ¬æ‰§è¡Œå‡ºé”™ (å¯èƒ½æ˜¯æ— å½±å“é”™è¯¯)"
    if [ -d "$MODEL_DIR/vllm" ] && [ -f "$MODEL_DIR/vllm/model.safetensors" ]; then
        touch "$MODEL_DIR/.conversion_complete"
        log "âœ… æ¨¡å‹è½¬æ¢å®Œæˆ"
    else
        warn "æ¨¡å‹è½¬æ¢ç»“æœä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥ $MODEL_DIR/vllm"
    fi
}

# ---------------- ç¯å¢ƒå‡†å¤‡ ----------------
log "âš™ï¸ æ£€æŸ¥ Python ç¯å¢ƒ..."
if command -v uv &> /dev/null; then
    PYTHON_CMD="uv run python"
    log "âœ… ä½¿ç”¨ uv è™šæ‹Ÿç¯å¢ƒæ‰§è¡Œ Python"
elif command -v python3 &> /dev/null; then
    PYTHON_CMD="python3"
    log "âœ… ä½¿ç”¨ç³»ç»Ÿ Python3"
else
    error "âŒ æœªæ£€æµ‹åˆ° Python3 æˆ– uvï¼Œè¯·æ£€æŸ¥é•œåƒç¯å¢ƒ"
    exit 1
fi

# æ£€æŸ¥ GPU æ˜¯å¦å¯è§
if command -v nvidia-smi &> /dev/null; then
    log "ğŸ§© GPU æ£€æµ‹æˆåŠŸ:"
    nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
else
    warn "âš ï¸ æœªæ£€æµ‹åˆ° NVIDIA GPUï¼Œå¯èƒ½åœ¨ CPU æ¨¡å¼ä¸‹è¿è¡Œ"
fi

# ---------------- ä¾èµ–å®‰è£… ----------------
log "ğŸ“¦ å®‰è£…ä¾èµ–..."
$PYTHON_CMD -m pip install --upgrade pip -i "$PYPI_MIRROR"
$PYTHON_CMD -m pip install modelscope huggingface_hub torch gradio vllm -i "$PYPI_MIRROR"

# ---------------- æ¨¡å‹ä¸‹è½½ä¸è½¬æ¢ ----------------
if [ "$DOWNLOAD_MODEL" = "1" ]; then
    if ! check_model_exists; then
        if [ "$VLLM_USE_MODELSCOPE" = "1" ]; then
            download_from_modelscope
        else
            download_from_huggingface
        fi
    else
        log "âœ… æ¨¡å‹å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½"
    fi
else
    warn "ğŸ›‘ æ¨¡å‹ä¸‹è½½å·²ç¦ç”¨"
fi

if [ "$CONVERT_MODEL" = "1" ]; then
    if [ ! -f "$MODEL_DIR/.conversion_complete" ]; then
        convert_model_format
    else
        log "âœ… æ¨¡å‹å·²è½¬æ¢ï¼Œè·³è¿‡"
    fi
fi

# ---------------- å¯åŠ¨æœåŠ¡ ----------------
log "ğŸš€ å¯åŠ¨ IndexTTS-2 vLLM API æœåŠ¡ (ç«¯å£: $PORT)"
exec $PYTHON_CMD /app/api_server.py \
    --model_dir "$MODEL_DIR" \
    --port "$PORT" \
    --gpu_memory_utilization "$GPU_MEMORY_UTILIZATION"
